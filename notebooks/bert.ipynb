{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "import torch\r\n",
    "import tensorflow as tf\r\n",
    "from torch.utils.data import TensorDataset , DataLoader, RandomSampler , SequentialSampler\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\r\n",
    "from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd \r\n",
    "import io \r\n",
    "import numpy as np \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from tqdm import tqdm, trange\r\n",
    "pd.set_option('max_colwidth', 400)\r\n",
    "pd.set_option('display.max_columns', None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(torch.cuda.is_available())\r\n",
    "print(torch.cuda.get_device_name())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TRAIN_TEXT_FILE_PTH=\"../data/raw/MELD/train/train_sent_emo.csv\"\r\n",
    "DEV_TEXT_FILE_PTH=\"../data/raw/MELD/dev/dev_sent_emo.csv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df=pd.read_csv(TRAIN_TEXT_FILE_PTH)\r\n",
    "dev_df=pd.read_csv(DEV_TEXT_FILE_PTH)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dev_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Seperating text and targets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_sentences=train_df.Utterance.values\r\n",
    "train_labels=train_df.Sentiment.values\r\n",
    "\r\n",
    "dev_sentences=dev_df.Utterance.values\r\n",
    "dev_labels=dev_df.Sentiment.values\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Encode labels\r\n",
    "from sklearn.preprocessing import OrdinalEncoder\r\n",
    "encoder=OrdinalEncoder()\r\n",
    "train_labels=encoder.fit_transform(train_labels.reshape(-1,1))\r\n",
    "dev_labels=encoder.transform(dev_labels.reshape(-1,1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\r\n",
    "\r\n",
    "tokenized_train_texts=[tokenizer.tokenize(sent) for sent in train_sentences]\r\n",
    "tokenized_dev_texts=[tokenizer.tokenize(sent) for sent in dev_sentences]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "max_len=0\r\n",
    "for i in tokenized_train_texts:\r\n",
    "    if len(i)>max_len:\r\n",
    "        max_len=len(i)\r\n",
    "print(max_len)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "MAX_LEN=128"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Numericalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_ids_train=[tokenizer.convert_tokens_to_ids(x) for x in tokenized_train_texts]\r\n",
    "print(tokenized_train_texts[0])\r\n",
    "print(input_ids_train[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_ids_dev=[tokenizer.convert_tokens_to_ids(x) for x in tokenized_dev_texts]\r\n",
    "print(tokenized_dev_texts[0])\r\n",
    "print(input_ids_dev[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_ids_train=pad_sequences(input_ids_train, maxlen=MAX_LEN, dtype='long',truncating='post', padding='post')\r\n",
    "input_ids_dev=pad_sequences(input_ids_dev, maxlen=MAX_LEN, dtype='long',truncating='post', padding='post')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating attention masks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_attention_masks=[]\r\n",
    "for seq in input_ids_train:\r\n",
    "    train_seq_mask=[float(i>0) for i in seq]\r\n",
    "    train_attention_masks.append(train_seq_mask)\r\n",
    "\r\n",
    "dev_attention_masks=[]\r\n",
    "for seq in input_ids_dev:\r\n",
    "    dev_seq_mask=[float(i>0) for i in seq]\r\n",
    "    dev_attention_masks.append(dev_seq_mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# To Tensors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_train=torch.tensor(input_ids_train, dtype=torch.long)\r\n",
    "input_val=torch.tensor(input_ids_dev, dtype=torch.long)\r\n",
    "label_train=torch.tensor(train_labels, dtype=torch.long)\r\n",
    "label_val=torch.tensor(dev_labels, dtype=torch.long)\r\n",
    "mask_train=torch.tensor(train_attention_masks, dtype=torch.float)\r\n",
    "mask_val=torch.tensor(dev_attention_masks, dtype=torch.float)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Batching and Iterator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data=TensorDataset(input_train, mask_train, label_train)\r\n",
    "val_data=TensorDataset(input_val, mask_val ,label_val)\r\n",
    "\r\n",
    "train_sampler=RandomSampler(train_data)\r\n",
    "train_dataloader=DataLoader(train_data, sampler=train_sampler, batch_size=32)\r\n",
    "val_sampler=RandomSampler(val_data)\r\n",
    "val_dataloader=DataLoader(val_data, sampler=val_sampler, batch_size=32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model config"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "configuration=BertConfig()\r\n",
    "model=BertModel(configuration)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(model.config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading Pretrained Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model=BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Param Optimization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "param_optimizer=list(model.named_parameters())\r\n",
    "no_decay=['bias', 'LayerNorm.weight']\r\n",
    "optimizer_grouped_parameters=[\r\n",
    "    {'params': [p for n,p in param_optimizer if not any(nd in n for nd in no_decay)],\r\n",
    "    'weight_decay_rate': 0.1,},\r\n",
    "    {'params':[p for n,p in param_optimizer if any(nd in n for nd in no_decay)],\r\n",
    "    'weight_decay_rate':0.0}\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer=AdamW(optimizer_grouped_parameters, lr=2e-5,eps = 1e-8 )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "def flat_accuracy(preds,labels):\r\n",
    "    pred_flat=np.argmax(preds,axis=1).flatten()\r\n",
    "    labels_flat=labels.flatten()\r\n",
    "    return np.sum(pred_flat==labels_flat) /len(labels_flat)\r\n",
    "\r\n",
    "def get_f1_score(preds, labels):\r\n",
    "    pred_flat=np.argmax(preds,axis=1).flatten()\r\n",
    "    labels_flat=labels.flatten()\r\n",
    "    return f1_score(labels_flat, pred_flat, average=\"weighted\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "torch.manual_seed(1)\r\n",
    "import random\r\n",
    "random.seed(1)\r\n",
    "import numpy as np\r\n",
    "np.random.seed(1)\r\n",
    "\r\n",
    "epochs=2\r\n",
    "total_steps=len(train_dataloader) *epochs\r\n",
    "scheduler=get_linear_schedule_with_warmup(optimizer,\r\n",
    "                                         num_warmup_steps=0,\r\n",
    "                                         num_training_steps=total_steps)\r\n",
    "\r\n",
    "t=[]\r\n",
    "train_loss_set=[]\r\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\r\n",
    "    model.cuda()\r\n",
    "    model.train()\r\n",
    "    tr_loss=0\r\n",
    "    nb_tr_examples, nb_tr_steps=0,0\r\n",
    "    \r\n",
    "    for step,batch in enumerate(train_dataloader):\r\n",
    "        batch=tuple(t.to('cuda') for t in batch)\r\n",
    "        b_input_ids,b_input_mask, b_labels=batch\r\n",
    "        optimizer.zero_grad()\r\n",
    "        outputs=model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\r\n",
    "        loss=outputs['loss']\r\n",
    "        train_loss_set.append(loss.item())\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        scheduler.step()\r\n",
    "\r\n",
    "        tr_loss+=loss.item()\r\n",
    "        nb_tr_examples+=b_input_ids.size(0)\r\n",
    "        nb_tr_steps+=1\r\n",
    "    print(\"train loss: {}\".format(tr_loss/nb_tr_steps))    \r\n",
    "\r\n",
    "    model.eval()\r\n",
    "    eval_los, eval_f1=0,0\r\n",
    "    nb_eval_steps, nb_eval_examples=0,0\r\n",
    "    \r\n",
    "    for batch in val_dataloader:\r\n",
    "        batch=tuple(t.to('cuda') for t in batch)        \r\n",
    "        b_input_ids,b_input_mask, b_labels=batch\r\n",
    "        \r\n",
    "        with torch.no_grad():\r\n",
    "            logits=model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask)\r\n",
    "        \r\n",
    "        logits=logits['logits'].detach().cpu().numpy()\r\n",
    "        label_ids=b_labels.to('cpu').numpy()\r\n",
    "        \r\n",
    "        tmp_eval_f1=get_f1_score(logits,label_ids)\r\n",
    "        eval_f1+=tmp_eval_f1\r\n",
    "        nb_eval_steps+=1\r\n",
    "\r\n",
    "    print(\"f1: {}\".format(eval_f1/nb_eval_steps))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(model.state_dict(), \"../models/bert_model_sentiment_f1_65.ckpt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(15,8))\r\n",
    "plt.title(\"Training Loss\")\r\n",
    "plt.xlabel(\"Batch\")\r\n",
    "plt.ylabel(\"Loss\")\r\n",
    "plt.plot(train_loss_set)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('multimodal-emotion-detection': conda)"
  },
  "interpreter": {
   "hash": "50ca60d7a2d95917e7105bd8b9444f0ad58e5f652b68bb3143648ecd032506df"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}