{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Audio Baseline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import glob\r\n",
    "import torchaudio\r\n",
    "import librosa\r\n",
    "import copy\r\n",
    "import torch\r\n",
    "import numpy as np\r\n",
    "from torch.utils.data import Dataset\r\n",
    "import pandas as pd\r\n",
    "from torch import nn\r\n",
    "from tqdm import tqdm_notebook \r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATA_FOLDER_PTH=os.path.join(os.getcwd(), os.pardir, 'data')\r\n",
    "\r\n",
    "\r\n",
    "# processed\r\n",
    "TRAIN_AUDIO_FOLDER_PTH=os.path.join(DATA_FOLDER_PTH, 'processed/MELD/train_wavs')\r\n",
    "TRAIN_TEXT_FILE_PTH=os.path.join(DATA_FOLDER_PTH, 'raw/MELD/train', 'train_sent_emo.csv')\r\n",
    "\r\n",
    "\r\n",
    "# pths\r\n",
    "org_train_audio_pths=glob.glob(os.path.join(TRAIN_AUDIO_FOLDER_PTH, '*.wav'))\r\n",
    "\r\n",
    "# making train and dev out of org_train\r\n",
    "split_idx=int(len(org_train_audio_pths)*0.8)\r\n",
    "train_audio_pths=org_train_audio_pths[:split_idx]\r\n",
    "val_audio_pths=org_train_audio_pths[split_idx:]\r\n",
    "\r\n",
    "print(len(train_audio_pths))\r\n",
    "print(len(val_audio_pths))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_text=pd.read_csv(TRAIN_TEXT_FILE_PTH)\r\n",
    "train_text.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def info_to_wav_name(dialogue_id, utterance_id):\r\n",
    "    return 'dia{}_utt{}.wav'.format(dialogue_id, utterance_id)\r\n",
    "\r\n",
    "def emotion_to_label(emotion):\r\n",
    "    if emotion=='neutral':\r\n",
    "        return 0\r\n",
    "    elif emotion=='surprise':\r\n",
    "        return 1\r\n",
    "    elif emotion=='fear':\r\n",
    "        return 2\r\n",
    "    elif emotion=='sadness':\r\n",
    "        return 3\r\n",
    "    elif emotion=='joy':\r\n",
    "        return 4\r\n",
    "    elif emotion=='disgust':\r\n",
    "        return 5\r\n",
    "    elif emotion=='anger':\r\n",
    "        return 6\r\n",
    "\r\n",
    "train_text['wav_name']=train_text.apply(lambda x: info_to_wav_name(x['Dialogue_ID'], x['Utterance_ID']), axis=1)\r\n",
    "train_text['label']=train_text.apply(lambda x: emotion_to_label(x['Emotion']), axis=1)\r\n",
    "train_text.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_text['Emotion'].unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wav_to_label=dict(zip(train_text['wav_name'], train_text['label']))\r\n",
    "print(wav_to_label['dia0_utt0.wav'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extraction Testing (librosa)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_length=10\r\n",
    "sr=8000\r\n",
    "sample, sample_rate=librosa.load(train_audio_pths[30], sr=sr)\r\n",
    "print(sample.shape)\r\n",
    "print(sample_rate)\r\n",
    "\r\n",
    "short_samples=librosa.util.fix_length(sample, sr * max_length)\r\n",
    "print(short_samples.shape)\r\n",
    "\r\n",
    "melSpectrum=librosa.feature.melspectrogram(short_samples.astype(np.float16), sr=sr, n_mels=40)\r\n",
    "print(melSpectrum.shape)\r\n",
    "logMelSpectrum=librosa.power_to_db(melSpectrum, ref=np.max)\r\n",
    "print(logMelSpectrum.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AudioDataset(Dataset):\r\n",
    "    def __init__(self, pths, wav_to_label, max_length=10, sr=8000):\r\n",
    "        self.pths=pths\r\n",
    "        self.wav_to_label=wav_to_label\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.pths)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        # not working!\r\n",
    "        sample, sample_rate=librosa.load(self.pths[idx], sr=sr)\r\n",
    "        short_samples=librosa.util.fix_length(sample, sr * max_length)\r\n",
    "        melSpectrum=librosa.feature.melspectrogram(short_samples.astype(np.float16), sr=sr, n_mels=40)\r\n",
    "        logMelSpectrum=librosa.power_to_db(melSpectrum, ref=np.max)\r\n",
    "        return logMelSpectrum, torch.tensor(self.wav_to_label[self.pths[idx].split('\\\\')[-1]], dtype=torch.long)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_ds=AudioDataset(train_audio_pths, wav_to_label)\r\n",
    "train_loader=torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\r\n",
    "\r\n",
    "val_ds=AudioDataset(val_audio_pths, wav_to_label)\r\n",
    "val_loader=torch.utils.data.DataLoader(val_ds, batch_size=32, shuffle=True, num_workers=4)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "next(iter(train_loader))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Model(nn.Module):\r\n",
    "    def __init__(self, input_size):\r\n",
    "        super(Model, self).__init__()\r\n",
    "        self.input_size = input_size\r\n",
    "        \r\n",
    "        self.fc1 = nn.Linear(self.input_size, 128)\r\n",
    "        self.bc1 = nn.BatchNorm1d(128)\r\n",
    "        \r\n",
    "        self.fc2 = nn.Linear(128, 128)\r\n",
    "        self.bc2 = nn.BatchNorm1d(128)\r\n",
    "        \r\n",
    "        self.fc3 = nn.Linear(128, 10)\r\n",
    "        \r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        # flatten\r\n",
    "        x = x.view((-1, self.input_size))\r\n",
    "        h = self.fc1(x)\r\n",
    "        h = self.bc1(h)\r\n",
    "        h = torch.relu(h)\r\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\r\n",
    "        \r\n",
    "        h = self.fc2(h)\r\n",
    "        h = self.bc2(h)\r\n",
    "        h = torch.relu(h)\r\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\r\n",
    "        \r\n",
    "        h = self.fc3(h)\r\n",
    "        out = torch.log_softmax(h,dim=1)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_features=157"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Model(n_features)\r\n",
    "# Track metrics when training\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\r\n",
    "n_epoch = 40\r\n",
    "train_losses = []\r\n",
    "val_losses = []\r\n",
    "best_val = np.inf\r\n",
    "best_model = copy.deepcopy(model)\r\n",
    "accuracies = []\r\n",
    "\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(device)\r\n",
    "model.to(device)\r\n",
    "\r\n",
    "# Train the model\r\n",
    "for epoch in range(n_epoch):\r\n",
    "    print('Epoch: ' + str(epoch + 1)) #Index at 0\r\n",
    "    # Training loop\r\n",
    "    model.train()\r\n",
    "    correct = 0\r\n",
    "    train_loss = 0\r\n",
    "    with tqdm_notebook(enumerate(train_loader), total=len(train_loader)) as pbar:\r\n",
    "        for batch_idx, (data, target) in pbar:\r\n",
    "            # Get Samples\r\n",
    "            data, target = data.to(device), target.to(device)\r\n",
    "\r\n",
    "            # Clear gradients\r\n",
    "            optimizer.zero_grad()\r\n",
    "\r\n",
    "            # Forward Propagation \r\n",
    "            y_pred = model(data) \r\n",
    "\r\n",
    "            # Error Computation\r\n",
    "            loss = F.cross_entropy(y_pred, target)\r\n",
    "\r\n",
    "            # Backpropagation\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "\r\n",
    "            # Track training loss\r\n",
    "            train_loss += loss.data.sum()\r\n",
    "            \r\n",
    "            #Added to look at training accuracy\r\n",
    "            pred = y_pred.argmax(dim=1, keepdim=True) # get the index of the max log-probability\r\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\r\n",
    "            \r\n",
    "            pbar.set_description(\"Current loss %.4f\" % (train_loss/(len(target)*(1+batch_idx))))\r\n",
    "            \r\n",
    "    train_losses.append(train_loss/len(train_loader.sampler.indices))\r\n",
    "    print('Training set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n",
    "        train_losses[-1], correct, len(train_loader.sampler.indices),\r\n",
    "        100. * correct / len(train_loader.sampler.indices)))\r\n",
    "    \r\n",
    "    # Validation Loop\r\n",
    "    model.eval()\r\n",
    "    val_loss = 0\r\n",
    "    correct = 0\r\n",
    "    with torch.no_grad():\r\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\r\n",
    "            data, target = data.to(device), target.to(device)\r\n",
    "            output = model(data)\r\n",
    "            val_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\r\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\r\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\r\n",
    "\r\n",
    "    val_loss /= len(val_loader.sampler.indices)\r\n",
    "    val_losses.append(val_loss)\r\n",
    "    if val_loss < best_val:\r\n",
    "        best_val = val_loss\r\n",
    "        best_model = copy.deepcopy(model)\r\n",
    "    accuracy = 100. * correct / len(val_loader.sampler.indices)\r\n",
    "    accuracies.append(accuracy)\r\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n",
    "        val_loss, correct, len(val_indices), accuracy))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('multimodal-emotion-detection': conda)"
  },
  "interpreter": {
   "hash": "50ca60d7a2d95917e7105bd8b9444f0ad58e5f652b68bb3143648ecd032506df"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}